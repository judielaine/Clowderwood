{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274a455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with the acurite raw data\n",
    "source = \"acurite\"\n",
    "stateDirs = \"rawCSVdirList\"\n",
    "\n",
    "dropColumns = ['Sensor Type','Accumulated Rain',\n",
    "       'Wind Speed', 'Wind Average', 'Wind Direction',\n",
    "       'Wired Sensor Temperature', 'Wired Sensor Humidity',\n",
    "       'Soil & Liquid Temperature', 'Water Detected', 'UV Index',\n",
    "       'Light Intensity', 'Measured Light', 'Lightning Strike Count',\n",
    "       'Lightning Closest Strike Distance','Feels Like',\n",
    "       'Wind Chill']\n",
    "\n",
    "dupeOnColumns = ['Sensor Name', 'Timestamp']\n",
    "splitCol = \"Sensor Name\"\n",
    "timeCol = \"Timestamp\"\n",
    "timestampFormat = '%Y/%m/%d %I:%M %p'\n",
    "locale = 'America/New_York'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742154b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This is the  path to the package & config dirs\n",
    "pyPath = '../'\n",
    "\n",
    "# importing python scripts\n",
    "pyAPath = os.path.abspath(pyPath)\n",
    "\n",
    "if pyAPath not in sys.path:\n",
    "    sys.path.append(os.path.abspath(pyPath))\n",
    "\n",
    "from clowderwoodpy import cw_start, getCWDataFileList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5468ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move functions to clowderwoodpy\n",
    "def load_csv_files(file_paths,dropCols=None,uniqueVals=None,timeCol=None,timeFormat=None,timeLoc=None):\n",
    "    \"\"\"Load multiple CSV files into a list of DataFrames.\"\"\"\n",
    "    dataframes = []\n",
    "    metaDict = {}\n",
    "    for file in file_paths:\n",
    "        fn = file[-20:]\n",
    "        metaDict[fn] = {}\n",
    "        metaDict[fn][\"full path\"] = file\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"file\"] = fn\n",
    "            metaDict[fn][\"raw count\"] = len(df)\n",
    "            dataframes.append(df)\n",
    "\n",
    "            if timeCol:\n",
    "                if timeFormat:\n",
    "                    df[timeCol] = pd.to_datetime(df[timeCol], format=timeFormat)\n",
    "\n",
    "                    # Extract most recent (max) and earliest (min) values\n",
    "                    metaDict[fn][\"earliest\"] = df['Timestamp'].max().isoformat()\n",
    "                    metaDict[fn][\"latest\"] = df['Timestamp'].min().isoformat()\n",
    "\n",
    "                    if timeLoc:\n",
    "                        df[timeCol]  = df[timeCol].dt.tz_localize(timeLoc,ambiguous=\"infer\")\n",
    "\n",
    "                else:\n",
    "                    raise MissingTimeFormat(f\"Can't convert column {timeCol} without source format.\")\n",
    "\n",
    "            if dropCols:\n",
    "                df.drop(dropCols, axis=1, inplace=True)\n",
    "            if uniqueVals:\n",
    "                if isinstance(uniqueVals,list):\n",
    "                    for col in uniqueVals:\n",
    "                        metaDict[fn][col+\" (unique)\"] = list(df[col].unique())\n",
    "                else: \n",
    "                    col = uniqueVals\n",
    "                    metaDict[fn][col+\" (unique)\"] = list(df[col].unique())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    return dataframes,metaDict\n",
    "\n",
    "# TODO perhaps add changing the index here? But not for acurite\n",
    "\n",
    "def remove_duplicates(dataframes,dupeColumns,sortCols=None):\n",
    "    \"\"\"Concatenate DataFrames and remove duplicate rows.\"\"\"\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    cleaned_df = combined_df.drop_duplicates(subset=dupeColumns, keep='first').copy()\n",
    "    if sortCols:\n",
    "        cleaned_df.sort_values(by=sortCols,inplace=True)\n",
    "\n",
    "    cleaned_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "def rollingOutlierDetection(rawSeries,label,rollWindow=\"2h\",minDeviation=None):\n",
    "    \"\"\"Find outliers .\"\"\"\n",
    "\n",
    "    # Create a rolling window of 1 hour (2 windows of 30 minutes around each point, excluding the point itself)\n",
    "    rolling_window = rawSeries.rolling(window=rollWindow, min_periods=1, center=True)\n",
    "\n",
    "    # Calculate rolling mean and std deviation\n",
    "    rolling_mean = rolling_window.mean()\n",
    "    rolling_std = rolling_window.std()\n",
    "\n",
    "    # Identify outliers (more than 2 standard deviations away from the rolling mean)\n",
    "    outliers = (rawSeries > rolling_mean + 2 * rolling_std) | (rawSeries < rolling_mean - 2 * rolling_std)\n",
    "\n",
    "    extremes  = (rawSeries > rolling_mean + 4 * rolling_std) | (rawSeries < rolling_mean - 4 * rolling_std)\n",
    "\n",
    "    resultsDict={\n",
    "        label : rawSeries,\n",
    "        'rolling mean' : rolling_mean,\n",
    "        'rolling std' :  rolling_std,\n",
    "        'outlier'  : outliers,\n",
    "        'extreme' : extremes,\n",
    "    }\n",
    "\n",
    "    if minDeviation:\n",
    "        # create new outliers and extremes\n",
    "        exceedMinDel = abs(rawSeries - rolling_mean) > minDeviation\n",
    "        resultsDict['outlier > min del'] = outliers & exceedMinDel\n",
    "        resultsDict['extreme > min del']  = extremes & exceedMinDel\n",
    "\n",
    "\n",
    "    resultsDF = pd.DataFrame(resultsDict)\n",
    "\n",
    "    return resultsDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0288a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to clowderwoodpy v 0.0.1 .\n",
      "\n",
      " This initialization has loaded a dictionary which contains the current \n",
      " raw, processed, and summarized data locations. The dictionary is\n",
      " organized by sensor or source classes, as the first level key:\n",
      "\t acurite\n",
      "\t rainGauge\n",
      "\n",
      " Each sensor has directories for different data processing states. These\n",
      " directories include the following, with the caution that not all sources\n",
      " have all data processing states. These are the second level key:\n",
      "\t rawCSVdirList\n",
      "\t summaryDir\n",
      "\t processedDir\n",
      "\t rawHTTPdirList\n",
      "\n",
      " In preparing the dictionary the paths are converted to absolute \n",
      " paths from the values recorded in ../config/clowderwoodDataFile.json .\n"
     ]
    }
   ],
   "source": [
    "cwDataDict, cwLabelsDict = cw_start(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9687010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from acurite , rawCSVdirList :\n",
      "\t 101 files.\n",
      "Overlap check:\n",
      "\tbefore = 2100940 \n",
      "\tafter  = 1800436\n"
     ]
    }
   ],
   "source": [
    "print (\"Data from\",source,\",\",stateDirs,\":\")\n",
    "fileList= getCWDataFileList(cwDataDict,source,stateDirs)\n",
    "print (\"\\t\",len(fileList),\"files.\")\n",
    "\n",
    "dfList, rawMetaDict = load_csv_files(fileList,\n",
    "                                     dropCols=dropColumns,\n",
    "                                     uniqueVals=\"Sensor Name\",\n",
    "                                     timeCol='Timestamp',timeFormat=timestampFormat)\n",
    "\n",
    "# Not setting the locale on the Timestamp column because infer fails, possibly because there\n",
    "# are multiple rows with same timestamp. Will try adding locale when separating into individual\n",
    "# \n",
    "\n",
    "cleanDF = remove_duplicates(dfList,dupeOnColumns,sortCols=dupeOnColumns)\n",
    "rawMetaDF = pd.DataFrame.from_dict(rawMetaDict, orient='index')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Overlap check:\\n\\tbefore =\",rawMetaDF[\"raw count\"].sum(),\n",
    "      \"\\n\\tafter  =\",len(cleanDF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038389dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Name 'Back Fence' had 48 ambiguous timestamps in the\n",
      "Timestamp, which did not survive conversion to America/New_York.\n",
      "\n",
      "Sensor Name 'Back Fence Retired 2' had 12 ambiguous timestamps in the\n",
      "Timestamp, which did not survive conversion to America/New_York.\n",
      "\n",
      "Sensor Name 'Garage' had 36 ambiguous timestamps in the\n",
      "Timestamp, which did not survive conversion to America/New_York.\n",
      "\n",
      "Sensor Name 'Greenhouse' had 24 ambiguous timestamps in the\n",
      "Timestamp, which did not survive conversion to America/New_York.\n",
      "\n",
      "Sensor Name 'In home  (w/display)' had 59 ambiguous timestamps in the\n",
      "Timestamp, which did not survive conversion to America/New_York.\n",
      "\n",
      "Sensor Name 'Raingauge' had 24 ambiguous timestamps in the\n",
      "Timestamp, which did not survive conversion to America/New_York.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO create splitCol_opt_indexTime function and move to clowderwoodpy\n",
    "\n",
    "# see <https://pandas.pydata.org/docs/user_guide/timeseries.html>\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype \n",
    "import matplotlib.pyplot as plt\n",
    "# def splitCol_opt_indexTime(dataframe,splitCol,timeCol=None,locale=None):\n",
    "#   \"\"\"\"\"\"\n",
    "splitMetaDict = {}\n",
    "splitDFDict = {}\n",
    "splitList = list(cleanDF[splitCol].unique())\n",
    "\n",
    "for s in splitList:\n",
    "    splitMetaDict[s] = {}\n",
    "    splitMetaDict[s][\"notes\"] = \"\"\n",
    "\n",
    "    idx = cleanDF[splitCol] == s\n",
    "    df = cleanDF.loc[idx].copy()\n",
    "    splitMetaDict[s][\"raw count\"] = len(df)\n",
    "\n",
    "    df.drop(splitCol, axis='columns', inplace=True)\n",
    "\n",
    "    if is_datetime64_any_dtype(df[timeCol]):\n",
    "\n",
    "        df.sort_values(by=timeCol, inplace=True)\n",
    "\n",
    "        if locale:\n",
    "            # Getting timestamp as timezone aware\n",
    "            df[timeCol] = df[timeCol].dt.tz_localize(locale, ambiguous='NaT', nonexistent='NaT')\n",
    "\n",
    "            # Drop rows with NaT values in 'Timestamp' column\n",
    "            df.dropna(subset=[timeCol], inplace=True)\n",
    "\n",
    "            delta = splitMetaDict[s][\"raw count\"] - len(df)\n",
    "            if delta != 0:\n",
    "                noteStr = f\"{splitCol} '{s}' had {delta} ambiguous timestamps in the\\n{timeCol}, which did not survive conversion to {locale}.\\n\"\n",
    "                print(noteStr)\n",
    "                splitMetaDict[s][\"notes\"] = splitMetaDict[s][\"notes\"]+noteStr\n",
    "\n",
    "        # Extract most recent (max) and earliest (min) values\n",
    "        splitMetaDict[s][\"latest entry\"] = df[timeCol].max().isoformat()\n",
    "        splitMetaDict[s][\"earliest entry\"] = df[timeCol].min().isoformat()\n",
    "\n",
    "        # Determine usual delta between timestamps\n",
    "\n",
    "        tempDF = df[[timeCol]].copy()\n",
    "        tempDF['interval'] = tempDF[timeCol].diff()\n",
    "        tempDF = tempDF.dropna(subset=['interval'])\n",
    "        intervalFreqSeries = tempDF['interval'].value_counts().sort_index()\n",
    "        usualInterval = intervalFreqSeries.idxmax()\n",
    "        splitMetaDict[s][\"usual interval\"] = usualInterval\n",
    "\n",
    "        # Characterize the remaining deltas \n",
    "\n",
    "        intervalFreqSeries.drop(usualInterval,axis='index',inplace=True)\n",
    "        splitMetaDict[s][\"gap count\"] =  intervalFreqSeries.sum()\n",
    "\n",
    "        idx = tempDF[\"interval\"] != usualInterval\n",
    "        splitMetaDict[s][\"gap df\"] = tempDF.loc[idx]\n",
    "\n",
    "        # Make the timeCol the index\n",
    "        df.set_index(timeCol,inplace=True)\n",
    "    else:\n",
    "        colType = str(df[timeCol].dtype)\n",
    "        raise AttributeError(f\"The \\'{timeCol}\\' column is not datetimelike but is {colType}:\\n\\t{e}\")\n",
    "    \n",
    "    splitDFDict[s] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b06a6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Temperature ( F )', 'Humidity ( RH )', 'Dew Point ( F )',\n",
       "       'Heat Index ( F )', 'Barometric Pressure ( INHG )', 'file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the range of dates in \n",
    "# 'Raingauge'? First seems to be 2023-10-21 10:30:00-04:00\n",
    "# 'Back Fence' 2019-11-16 00:00:00-05:00 - 2025-04-02 21:55:00-04:00\n",
    "# 'Back Fence Retired 2' 2023-10-31 00:00:00-04:00 - 2023-11-05 16:15:00-05:00\n",
    "\n",
    "splitDFDict['Back Fence'].sort_values(by='Timestamp',ascending=True) \n",
    "splitDFDict['Back Fence'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f07e1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 25                        30  \\\n",
      "Timestamp                                                       \n",
      "2020                            NaT                       NaT   \n",
      "2021      2021-04-03 07:15:00-04:00 2021-04-03 08:30:00-04:00   \n",
      "2022                            NaT                       NaT   \n",
      "2023                            NaT                       NaT   \n",
      "2024                            NaT                       NaT   \n",
      "2025                            NaT                       NaT   \n",
      "\n",
      "                                 32                        40  \\\n",
      "Timestamp                                                       \n",
      "2020      2020-05-10 06:15:00-04:00 2020-05-12 07:45:00-04:00   \n",
      "2021      2021-04-23 07:30:00-04:00 2021-05-14 07:00:00-04:00   \n",
      "2022      2022-04-20 07:35:00-04:00 2022-04-21 07:30:00-04:00   \n",
      "2023                            NaT 2023-05-04 06:35:00-04:00   \n",
      "2024      2024-04-23 07:30:00-04:00 2024-04-25 07:10:00-04:00   \n",
      "2025                            NaT                       NaT   \n",
      "\n",
      "                                 45                        50  \\\n",
      "Timestamp                                                       \n",
      "2020      2020-05-13 05:40:00-04:00 2020-05-13 08:25:00-04:00   \n",
      "2021      2021-05-16 05:20:00-04:00 2021-05-22 06:25:00-04:00   \n",
      "2022      2022-05-10 06:15:00-04:00 2022-05-11 06:45:00-04:00   \n",
      "2023      2023-05-05 06:20:00-04:00 2023-06-09 07:05:00-04:00   \n",
      "2024      2024-06-01 07:25:00-04:00 2024-06-01 08:10:00-04:00   \n",
      "2025      2025-04-02 01:30:00-04:00 2025-04-02 07:50:00-04:00   \n",
      "\n",
      "                                 55                        60  \\\n",
      "Timestamp                                                       \n",
      "2020      2020-06-02 04:30:00-04:00 2020-06-15 08:15:00-04:00   \n",
      "2021      2021-06-01 08:00:00-04:00 2021-06-02 07:45:00-04:00   \n",
      "2022      2022-06-20 07:35:00-04:00 2022-06-22 07:20:00-04:00   \n",
      "2023      2023-06-14 07:10:00-04:00 2023-06-18 07:45:00-04:00   \n",
      "2024      2024-06-08 07:15:00-04:00 2024-06-14 06:15:00-04:00   \n",
      "2025      2025-04-02 10:10:00-04:00 2025-04-02 12:35:00-04:00   \n",
      "\n",
      "                                 65                        70  \n",
      "Timestamp                                                      \n",
      "2020      2020-06-27 07:30:00-04:00 2020-06-30 06:45:00-04:00  \n",
      "2021      2021-06-06 07:30:00-04:00 2021-06-15 07:35:00-04:00  \n",
      "2022      2022-06-26 07:40:00-04:00 2022-06-30 08:10:00-04:00  \n",
      "2023      2023-06-29 07:30:00-04:00 2023-06-30 08:40:00-04:00  \n",
      "2024      2024-06-21 08:15:00-04:00 2024-06-29 06:50:00-04:00  \n",
      "2025      2025-04-02 21:55:00-04:00 2025-04-02 21:55:00-04:00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the temperature thresholds\n",
    "thresholds = [25, 30, 32, 40, 45, 50, 55, 60, 65, 70]\n",
    "\n",
    "# Filter the DataFrame for the specified date range (April 1 to June 30)\n",
    "filtered_df = splitDFDict['Back Fence'].loc[\n",
    "    (splitDFDict['Back Fence'].index.month >= 4) & \n",
    "    (splitDFDict['Back Fence'].index.month <= 6)\n",
    "]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each threshold\n",
    "for threshold in thresholds:\n",
    "    # Filter for rows where the temperature is at or below the threshold\n",
    "    below_threshold = filtered_df[filtered_df['Temperature ( F )'] <= threshold]\n",
    "    \n",
    "    # Group by year and find the last date for each year\n",
    "    last_dates = below_threshold.groupby(below_threshold.index.year).apply(\n",
    "        lambda group: group.index.max()\n",
    "    )\n",
    "    \n",
    "    # Store the results\n",
    "    results[threshold] = last_dates\n",
    "\n",
    "# Convert the results dictionary to a DataFrame for better readability\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbc019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (day2day)",
   "language": "python",
   "name": "day2day"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
