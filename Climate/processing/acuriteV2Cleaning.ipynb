{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: take the acurite export files from v2 and create easily reviewed summaries and easily loaded monthly files.  \n",
    "\n",
    "Output is split into directories for each property listed in `properties`. \n",
    "This decouples the files from the properties they record, allowing analysis of . \n",
    "In the property directory, the output is in a subdirectory for the `major` version of the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect files for cleaning (generic process)\n",
    "\n",
    "Cleaning considers all files in the `inputDir` directory compared against the list of `cleanedFiles`, which records all the files that have been completely processed and last version used in processing. \n",
    "\n",
    "Depending on the settings of the current versions and the booleans `minorRerun` and `allRerun`, the system will not reprocess files that have been successfully cleaned. \n",
    "\n",
    "The versioning method is informed by [semantic versioning](https://en.wikipedia.org/wiki/Software_versioning#Degree_of_compatibility)  When the major version of this cleaning notebook changes, the next run should load all the files and reprocess, placing the output in a new subdirectory for that property. Future development may use the minor number for changes that are compatible with existing consumers of the cleaned files. (More data in the report, additional export files.)  The patch version may be used for keeping track with development. \n",
    "\n",
    "## Merging details\n",
    "\n",
    "The process loads each file into a dataframe, renames the columns to `inputCols` and drops all the columns in `dropCols`. Duplicate data -- all columns match -- is dropped.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- That the file suffix `fileType` is an exact match (case sensitive)\n",
    "- That the files are comma separated value files that can be directly read into a data frame.\n",
    "- That the files have headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Collect files for cleaning: initialization\n",
    "# ---------------------------------------------------------------------#\n",
    "major = 0\n",
    "minor = 0 # Downstream notebooks can use previous minor versions.  \n",
    "patch = 0 # Only internal processing changes that should be tracked\n",
    "\n",
    "# Should files be reprocessed if the minor version is different?\n",
    "minorRerun = True \n",
    "\n",
    "# Should all files be reprocessed no matter what versions?\n",
    "allRerun = True\n",
    "\n",
    "#inputDir = '../Acurite.v2'\n",
    "inputDir = '../DeleteMe/'\n",
    "fileType = '.csv'\n",
    "cleanedFilesRecord = './acuritev2Cleaning.json'\n",
    "\n",
    "inputCols = [\"Name\",\"X00\",\"Timestamp\",\"Temperature_F\",\"Humidity_pct\",\n",
    "             \"Dew_Point_F\",\"Heat_Index_F\",\"X01\",\"X02\",\"Pressure_inHg\",\n",
    "             \"X03\",\"X04\",\"X05\",\"X06\",\"X07\",\"X08\",\"X09\",\"X10\",\"X11\",\"X12\",\n",
    "             \"X13\",\"X14\",\"X15\"]\n",
    "dropCols = [\"X00\",\"X01\",\"X02\",\"X03\",\"X04\",\"X05\",\"X06\",\"X07\",\"X08\",\"X09\",\n",
    "            \"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\"]\n",
    "\n",
    "\n",
    "now = pd.Timestamp.utcnow().isoformat()\n",
    "Status = collections.namedtuple('Status', ['major', 'minor',\n",
    "                               'patch','time']) \n",
    "currentStatus = Status(major,minor,patch,now)\n",
    "def statusStmt(myStatus):\n",
    "    \"\"\"Return a string from the namedtuple status\"\"\"\n",
    "    r = str(myStatus[0])+\".\"+str(myStatus[1])+\".\"+str(myStatus[2])+\" \"+now\n",
    "    return r\n",
    "\n",
    "report = \"Status: \"+statusStmt(currentStatus)+\".\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the files\n",
    "\n",
    "\n",
    "- Persist important details\n",
    "  - first and last observations of sensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Cleaning the files: initialization\n",
    "# ---------------------------------------------------------------------#\n",
    "sensorCol = \"Name\"\n",
    "sensorsRecord = '../sensorHistory.json'\n",
    "History = collections.namedtuple('History', ['earliest','latest','status']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting the cleaned data \n",
    "\n",
    "- Create output directories, if needed, labeled with the major version.\n",
    "- Convert the timestamp to a standard.\n",
    "- Separate on the `timestampCol` column into `period` blocks.\n",
    "- A block is considered complete if there is a record in the first and last `subPeriod` in the period.\n",
    "- Output with an ISO standard timestamp `isoTimestampCol`\n",
    "- Generate a run report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Persisting the cleaned data: initialization\n",
    "# ---------------------------------------------------------------------#\n",
    "stop = False\n",
    "outputPath = \"../\"\n",
    "# These are the general output directories.\n",
    "properties = ['Temperature','Humidity','Pressure']\n",
    "# Columns required in all output (other than timestamp)\n",
    "keyCols = [\"Name\"]\n",
    "# Dictionary of the columns appropirate for each propery\n",
    "propertyCols = {\n",
    "    'Temperature': [\"Temperature_F\",\"Heat_Index_F\"],\n",
    "    'Humidity': [\"Humidity_pct\",\"Dew_Point_F\"],\n",
    "    'Pressure': [\"Pressure_inHg\"]\n",
    "}\n",
    "\n",
    "timestampCol = 'Timestamp'\n",
    "\n",
    "period = \"month\"\n",
    "subPeriod = \"day\"\n",
    "\n",
    "# This will be the new column\n",
    "isoTimestampCol = 'ISO_Timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Collect files for cleaning: execution\n",
    "# ---------------------------------------------------------------------#\n",
    "\n",
    "# newFiles lists all the available input files, initially.\n",
    "input = os.listdir(inputDir)\n",
    "newFiles = []\n",
    "for f in input:\n",
    "    if f.endswith(fileType):\n",
    "        newFiles.append(f)\n",
    "\n",
    "report = report+str(len(newFiles))+\" \"+fileType+\" files in the \"+inputDir+\" directory.\\n\\n\"\n",
    "\n",
    "# cleanedFiles: a dictionary where the key is the input file name and \n",
    "# the output is a namedtuple Status of major, minor, patch, and \n",
    "# execution timestamp.\n",
    "try:    \n",
    "    with open(cleanedFilesRecord) as cf:\n",
    "        cleanedFiles = json.load(cf)\n",
    "        report = report+str(len(cleanedFiles))+\" files already processed:\\n\"      \n",
    "        for f in cleanedFiles:\n",
    "            cleanedFiles[f] = Status(*cleanedFiles[f])\n",
    "            report = report+f+\": \\n\\t\"+statusStmt(cleanedFiles[f])+\"\\n\"\n",
    "                     \n",
    "except FileNotFoundError:\n",
    "    cleanedFiles = {}\n",
    "    report = report+\"No cleaned files.\\n\"\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    raise\n",
    "    \n",
    "# If a file has been cleaned by an acceptable version of the processing \n",
    "# script, it will be removed from the new files list.\n",
    "reportRR = \"\"\n",
    "if allRerun:\n",
    "    reportRR = '\\n\\nNonetheless, all files will be rerun.\\n\\n'\n",
    "else:\n",
    "    if minorRerun:\n",
    "        reportRR = str(major)+\".\"+str(minor)\n",
    "    else:\n",
    "        reportRR = str(major)\n",
    "        \n",
    "    reportRR = '\\nAll files processed with a version earlier than '+reportRR+' will be rerun.\\n'\n",
    "    \n",
    "report = report+reportRR\n",
    "\n",
    "processFilesReport = \"\"\n",
    "processFiles = []\n",
    "\n",
    "for f in newFiles:\n",
    "    if f in cleanedFiles:\n",
    "        if  allRerun:\n",
    "            processFilesReport=processFilesReport+\"\\t\"+f+\"\\n\"\n",
    "            processFiles.append(f)\n",
    "        elif cleanedFiles[f].major < major:\n",
    "            processFilesReport=processFilesReport+\"\\t\"+f+\"\\n\"\n",
    "            processFiles.append(f)\n",
    "        elif minorRerun and cleanedFiles[f].minor < minor:\n",
    "            processFilesReport=processFilesReport+\"\\t\"+f+\"\\n\"\n",
    "            processFiles.append(f)\n",
    "    else:\n",
    "        processFilesReport=processFilesReport+\"\\t\"+f+\"\\n\"\n",
    "        processFiles.append(f)\n",
    "\n",
    "if len(processFiles) > 0:\n",
    "    report = report+\"The following \"+str(len(processFiles))+\" files are included in the cleaning:\\n\"\n",
    "    report = report+processFilesReport\n",
    "else:\n",
    "    report = report+\"No files to be cleaned.\\n\"\n",
    "\n",
    "# Import each new file into a dataframe, relabel the columns, pop out \n",
    "# the undesired columns and add to a list.\n",
    "processedDF = []\n",
    "\n",
    "for f in processFiles:\n",
    "    df = pd.read_csv(inputDir+f)\n",
    "    df.columns = inputCols\n",
    "    for c in dropCols:\n",
    "        df.pop(c)\n",
    "    cleanedFiles[f] = currentStatus\n",
    "    processedDF.append(df)\n",
    "    \n",
    "# Concatenate the list of dataframes into one frame, removing dupes.\n",
    "try:\n",
    "    inputDF = pd.concat(processedDF,ignore_index=True)\n",
    "    report = report+\"\\nThere are \"+str(len(inputDF))+\" rows in the sum of all the files. \"\n",
    "    inputDF.drop_duplicates(inplace=True)\n",
    "    report = report+\"After removing duplicates, there are \"+str(len(inputDF))+\" rows.\\n\"\n",
    "except ValueError:\n",
    "    print(\"No input files to process\") \n",
    "    with open('run'+pd.Timestamp.utcnow().strftime('%Y%M%dT%H%M')+'.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    stop = True\n",
    "\n",
    "\n",
    "# inputDF is the concatenation of all dataframes. \n",
    "# print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Cleaning the files: processing\n",
    "# ---------------------------------------------------------------------#\n",
    "if not stop:\n",
    "# Record when sensors were seen \n",
    "    try:    \n",
    "        with open(sensorsRecord) as sf:\n",
    "            sensors = json.load(sf)      \n",
    "            for s in sensors:\n",
    "                sensors[s] = History(*sensors[s])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        sensors = {}\n",
    "    except JSONDecodeError: # TODO why doesn't this work?\n",
    "        sensors = {}\n",
    "        copyfile(sensorsRecord, \n",
    "                 sensorsRecord+pd.Timestamp.utcnow().strftime('%Y%M%dT%H%M')+'.txt')\n",
    "    except:\n",
    "        print(\"Unexpected error\")\n",
    "        raise\n",
    "\n",
    "    sensorGroupDF = inputDF.groupby(sensorCol)\n",
    "    firstSeries = sensorGroupDF[timestampCol].min()\n",
    "    lastSeries = sensorGroupDF[timestampCol].max()\n",
    "    sensorList = lastSeries.index\n",
    "\n",
    "    for s in sensorList:\n",
    "        if s in sensors:  \n",
    "            print(\"Was:\",sensors[s])\n",
    "            if firstSeries[s] =< pd.to_datetime(sensors[s].earliest):\n",
    "                sensors[s].earliest = firstSeries[s].isoformat()\n",
    "                sensors[s].status = currentStatus\n",
    "\n",
    "            if lastSeries[s] >= pd.to_datetime(sensors[s].latest):\n",
    "                sensors[s].latest = lastSeries[s].isoformat()\n",
    "                sensors[s].status = currentStatus\n",
    "            print(\"Now:\",sensors[s])\n",
    "\n",
    "        else:\n",
    "            sensors[s] = History(firstSeries[s].isoformat(),lastSeries[s].isoformat(),currentStatus)\n",
    "\n",
    "    with open(sensorsRecord, 'w') as outfile:\n",
    "        json.dump(sensors, outfile)\n",
    "\n",
    "    report = report+\"\\n\\nDocumented the sensor history in \"+sensorsRecord+\".\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Persisting the cleaned data: execution\n",
    "# ---------------------------------------------------------------------#\n",
    "if not stop:\n",
    "    # By convention, the output is stored one directory up from this  \n",
    "    # cleaning notebook in a directory named for the property measured.\n",
    "    outputDir = {}\n",
    "    versionDir = 'cleaned.v'+str(major)\n",
    "    for p in properties:\n",
    "        # https://docs.python.org/3/library/os.html?highlight=os%20makedirs#os.makedirs \n",
    "        # Absolute path because documentation indicates risk with using \"../\"\n",
    "        pp = os.path.abspath(outputPath+p) \n",
    "        outputDir[p] = pp+\"/\"+versionDir\n",
    "        os.makedirs(outputDir[p], exist_ok=True)\n",
    "\n",
    "\n",
    "    record = record + \"\\nSplitting the input into files by \"+period+\".\\n\"\n",
    "    if period == 'year':\n",
    "        periodFmt = '%Y'\n",
    "    elif period == 'month':\n",
    "        periodFmt = '%Y-%m'\n",
    "    \n",
    "    inputDF['Period'] = inputDF[timestampCol].map(lambda x: x.strftime(periodFmt))\n",
    "    periodList = inputDF['Period'].unique()\n",
    "    PeriodDF = inputDF.groupby('Period')\n",
    "    countSeries = PeriodDF[timestampCol].count()\n",
    "    r = \"\\nTime period\\tCount of records\\n\"\n",
    "    for s,c in countSeries.iteritems():\n",
    "        r=r+s+\"\\t\\t\"+str(c)+\"\\n\"\n",
    "        \n",
    "    record = record + r\n",
    "    \n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#selecting-a-group\n",
    "\n",
    "# TODO How to verify period is complete OR load in previous months to merge?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Collect files for cleaning: persist success\n",
    "# ---------------------------------------------------------------------#\n",
    "# LAST CELL\n",
    "\n",
    "with open('run'+pd.Timestamp.utcnow().strftime('%Y%M%dT%H%M')+'.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "    \n",
    "# Record the completed files\n",
    "#with open(cleanedFilesRecord, 'w') as outfile:\n",
    "    #json.dump(cleanedFiles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PeriodDF = inputDF.groupby('Period')\n",
    "countSeries = PeriodDF[timestampCol].count()\n",
    "r = \"\\nTime period\\tCount of records\\n\"\n",
    "for s,c in countSeries.iteritems():\n",
    "    r=r+s+\"\\t\\t\"+str(c)+\"\\n\"\n",
    "    \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
