{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: take the acurite export files from v2 and create easily reviewed summaries and easily loaded monthly files.  \n",
    "\n",
    "Output is split into directories for each property listed in `properties`. \n",
    "This decouples the files from the properties they record, allowing analysis of . \n",
    "In the property directory, the output is in a subdirectory for the `major` version of the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect files for cleaning (generic process)\n",
    "\n",
    "Cleaning considers all files in the `inputDir` directory compared against the list of `cleanedFiles`, which records all the files that have been completely processed and last version used in processing. \n",
    "\n",
    "Depending on the settings of the current versions and the booleans `minorRerun` and `allRerun`, the system will not reprocess files that have been successfully cleaned. \n",
    "\n",
    "The versioning method is informed by [semantic versioning](https://en.wikipedia.org/wiki/Software_versioning#Degree_of_compatibility)  When the major version of this cleaning notebook changes, the next run should load all the files and reprocess, placing the output in a new subdirectory for that property. Future development may use the minor number for changes that are compatible with existing consumers of the cleaned files. (More data in the report, additional export files.)  The patch version may be used for keeping track with development. \n",
    "\n",
    "## Merging details\n",
    "\n",
    "The process loads each file into a dataframe, renames the columns to `inputCols` and drops all the columns in `dropCols`. Duplicate data -- all columns match -- is dropped.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- That the file suffix `fileType` is an exact match (case sensitive)\n",
    "- That the files are comma separated value files that can be directly read into a data frame.\n",
    "- That the files have headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Collect files for cleaning: initialization\n",
    "# ---------------------------------------------------------------------#\n",
    "major = 0\n",
    "minor = 0 # Downstream notebooks can use previous minor versions.  \n",
    "patch = 0 # Only internal processing changes that should be tracked\n",
    "\n",
    "# Should files be reprocessed if the minor version is different?\n",
    "minorRerun = False \n",
    "\n",
    "# Should all files be reprocessed no matter what versions?\n",
    "allRerun = True\n",
    "\n",
    "#inputDir = '../Acurite.v2'\n",
    "inputDir = '../DeleteMe/'\n",
    "fileType = '.csv'\n",
    "cleanedFilesRecord = './acuritev2.json'\n",
    "\n",
    "inputCols = [\"Name\",\"X00\",\"Timestamp\",\"Temperature_F\",\"Humidity_pct\",\n",
    "             \"Dew_Point_F\",\"Heat_Index_F\",\"X01\",\"X02\",\"Pressure_inHg\",\n",
    "             \"X03\",\"X04\",\"X05\",\"X06\",\"X07\",\"X08\",\"X09\",\"X10\",\"X11\",\"X12\",\n",
    "             \"X13\",\"X14\",\"X15\"]\n",
    "dropCols = [\"X00\",\"X01\",\"X02\",\"X03\",\"X04\",\"X05\",\"X06\",\"X07\",\"X08\",\"X09\",\n",
    "            \"X10\",\"X11\",\"X12\",\"X13\",\"X14\",\"X15\"]\n",
    "\n",
    "\n",
    "now = pd.Timestamp.utcnow().isoformat()\n",
    "Status = collections.namedtuple('Status', ['majorStatus', 'minorStatus',\n",
    "                               'patchStatus','timestampStatus']) \n",
    "currentStatus = Status(major,minor,patch,now)\n",
    "def statusStmt(myStatus):\n",
    "    r = str(myStatus[0])+\".\"+str(myStatus[1])+\".\"+str(myStatus[2])+\" \"+now\n",
    "    return r\n",
    "\n",
    "report = \"Status: \"+statusStmt(currentStatus)+\".\\n\\n\"\n",
    "print(type(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the files\n",
    "\n",
    "\n",
    "- Persist important details\n",
    "  - first and last observations of sensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Cleaning the files: initialization\n",
    "# ---------------------------------------------------------------------#\n",
    "sensorRecord = '../sensorHistory.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting the cleaned data \n",
    "\n",
    "- Create output directories, if needed, labeled with the major version.\n",
    "- Convert the timestamp to a standard.\n",
    "- Separate on the `timestampCol` column into `period` blocks.\n",
    "- A block is considered complete if there is a record in the first and last `subPeriod` in the period.\n",
    "- Output with a new ISO standard timestamp `isoTimestampCol`\n",
    "- Generate a run report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Persisting the cleaned data: initialization\n",
    "# ---------------------------------------------------------------------#\n",
    "outputPath = \"../\"\n",
    "# These are the general output directories.\n",
    "properties = ['Temperature','Humidity','Pressure']\n",
    "\n",
    "# The input timestamp format is \"2019/11/16 12:00 AM\"\n",
    "timestampCol = 'Timestamp'\n",
    "isoTimestampCol = 'ISO_Timestamp'\n",
    "period = \"month\"\n",
    "subPeriod = \"day\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Collect files for cleaning: execution\n",
    "# ---------------------------------------------------------------------#\n",
    "\n",
    "# newFiles lists all the available input files, initially.\n",
    "input = os.listdir(inputDir)\n",
    "newFiles = []\n",
    "for f in input:\n",
    "    if f.endswith(fileType):\n",
    "        newFiles.append(f)\n",
    "\n",
    "report = report+str(len(newFiles))+\" \"+fileType+\" files in the \"+inputDir+\" directory.\\n\\n\"\n",
    "\n",
    "# cleanedFiles: a dictionary where the key is the input file name and \n",
    "# the output is a namedtuple Status of major, minor, patch, and \n",
    "# execution timestamp.\n",
    "try:    \n",
    "    with open(cleanedFilesRecord) as f:\n",
    "        cleanedFiles = json.load(f)\n",
    "        report = report+str(len(cleanedFiles))+\" files already processed:\\n\"      \n",
    "        for ff in cleanedFiles:\n",
    "            cleanedFiles[ff] = Status(*cleanedFiles[ff])\n",
    "            report = report+ff+\": \\n\\t\"+statusStmt(cleanedFiles[ff])+\"\\n\"\n",
    "                     \n",
    "except FileNotFoundError:\n",
    "    cleanedFiles = {}\n",
    "    report = report+\"No cleaned files.\\n\"\n",
    "except:\n",
    "    print(\"Unexpected error\")\n",
    "    raise\n",
    "    \n",
    "# If a file has been cleaned by an acceptable version of the processing \n",
    "# script, it will be removed from the new files list.\n",
    "\n",
    "#TODO put in stop processing controls\n",
    "newFilesReport = \"\"\n",
    "\n",
    "if not allRerun:\n",
    "    for f in newFiles:\n",
    "        if f in cleanedFiles:\n",
    "            next\n",
    "            # check dictionary tuple\n",
    "            #if checked file major >= major :\n",
    "            #    newFiles.remove(f)\n",
    "            #if checkminor = True AND checked file minor >= minor\n",
    "            #    newFiles.remove(f)\n",
    "       \n",
    "        newFilesReport=newFilesReport+\"\\t\"+f+\"\\n\"\n",
    "        \n",
    "else:\n",
    "    report = report+'\\n\\nNonetheless, all files will be rerun.\\n\\n'\n",
    "    \n",
    "\n",
    "report = report+\"The following \"+str(len(newFiles))+\" files are included in the cleaning:\\n\"\n",
    "report = report+newFilesReport\n",
    "\n",
    "# Import each new file into a dataframe, relabel the columns, pop out \n",
    "# the undesired columns and add to a list.\n",
    "newFilesDF = []\n",
    "\n",
    "for f in newFiles:\n",
    "    df = pd.read_csv(inputDir+f)\n",
    "    df.columns = inputCols\n",
    "    for c in dropCols:\n",
    "        df.pop(c)\n",
    "    cleanedFiles[f] = currentStatus\n",
    "    newFilesDF.append(df)\n",
    "    \n",
    "# Concatenate the list of dataframes into one frame, removing dupes.\n",
    "inputDF = pd.concat(newFilesDF,ignore_index=True)\n",
    "\n",
    "report = report+\"\\nThere are \"+str(len(inputDF))+\" rows in the sum of all the files. \"\n",
    "inputDF.drop_duplicates(inplace=True)\n",
    "report = report+\"After removing duplicates, there are \"+str(len(inputDF))+\" rows.\\n\"\n",
    "\n",
    "# inputDF is the concatenation of all dataframes. \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Persisting the cleaned data: execution\n",
    "# ---------------------------------------------------------------------#\n",
    "# Convert the timestamp column `timestampCol` to the timestamp format \n",
    "## inputDF[timestampCol] = pd.to_datetime(inputDF[timestampCol])\n",
    "\n",
    "# TODO How to split into period blocks\n",
    "\n",
    "# TODO How to verify period is complete\n",
    "\n",
    "# By convention, the output is stored one directory up from this  \n",
    "# cleaning notebook in a directory named for the property measured.\n",
    "outputDir = {}\n",
    "versionDir = 'cleaned.v'+str(major)\n",
    "for p in properties:\n",
    "    # https://docs.python.org/3/library/os.html?highlight=os%20makedirs#os.makedirs \n",
    "    # Absolute path because documentation indicates risk with using \"../\"\n",
    "    pp = os.path.abspath(outputPath+p) \n",
    "    outputDir[p] = pp+\"/\"+versionDir\n",
    "    os.makedirs(outputDir[p], exist_ok=True)\n",
    "\n",
    "    \n",
    "# TODO make final report\n",
    "with open('run'+pd.Timestamp.utcnow().strftime('%Y%M%dT%H%M')+'.txt', 'w') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------#\n",
    "# Collect files for cleaning: persist success\n",
    "# ---------------------------------------------------------------------#\n",
    "# LAST CELL\n",
    "# Record the completed files\n",
    "with open(cleanedFilesRecord, 'w') as outfile:\n",
    "    json.dump(cleanedFiles, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
